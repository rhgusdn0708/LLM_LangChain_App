{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14e85d18",
   "metadata": {},
   "source": [
    "### 2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Groq 모델 설정 (이전 노트북 파일에서 사용된 설정 재활용)\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# PromptTemplate 정의\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"당신은 사용자가 질문한 내용을 기반으로 유용한 정보를 제공하는 친절한 챗봇입니다.\n",
    "    질문: {question}\n",
    "    이 질문에 대해 자세하고 유익한 답변을 제공해주세요.\"\"\"\n",
    ")\n",
    "\n",
    "# RunnableParallel을 사용하여 'question' 인풋을 처리\n",
    "# 'question' 키의 값을 그대로 전달 (RunnablePassthrough)\n",
    "# 그 값을 prompt에 연결하고, prompt 결과를 model에 연결\n",
    "chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "# 인보커를 사용하여 질문에 대한 응답 받기\n",
    "question_input = \"LangChain이란 무엇인가요?\"\n",
    "response = chain.invoke(question_input)\n",
    "\n",
    "print(f\"질문: {question_input}\")\n",
    "print(f\"답변: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3d15be",
   "metadata": {},
   "source": [
    "### 2-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f2e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Groq 모델 설정 (이전 노트북 파일에서 사용된 설정 재활용)\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# PromptTemplate 정의 (ChatPromptTemplate 사용)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 사용자에게 유용한 정보를 제공하는 AI 어시스턴트입니다.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 체인 정의: prompt -> model -> output_parser\n",
    "# | 연산자를 사용하여 구성 요소를 연결합니다.\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# 체인 실행\n",
    "question_input = \"대한민국의 수도는 어디인가요?\"\n",
    "response_content = chain.invoke({\"question\": question_input})\n",
    "\n",
    "print(f\"질문: {question_input}\")\n",
    "print(f\"답변: {response_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec82e248",
   "metadata": {},
   "source": [
    "### 2-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6524daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field # Pydantic 모델 정의를 위해 임포트\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Groq 모델 설정 (이전 노트북 파일에서 사용된 설정 재활용)\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# JsonOutputParser에 사용할 Pydantic 모델 정의\n",
    "# LLM이 생성할 JSON 응답의 스키마를 정의합니다.\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"사람의 이름\")\n",
    "    age: int = Field(description=\"사람의 나이\")\n",
    "    city: str = Field(description=\"사람이 사는 도시\")\n",
    "\n",
    "# JsonOutputParser 초기화, Pydantic 모델 전달\n",
    "parser = JsonOutputParser(pydantic_object=Person)\n",
    "\n",
    "# PromptTemplate 정의\n",
    "# LLM에게 JSON 형식으로 응답하도록 지시하고, 파서의 형식 지침을 포함합니다.\n",
    "# 주의: 이 프롬프트는 LLM에게 명확하게 JSON 출력을 요청해야 합니다.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 사용자의 정보를 기반으로 JSON 형식의 정보를 제공하는 어시스턴트입니다.\\n{format_instructions}\"),\n",
    "    (\"user\", \"이름이 {name}이고 나이가 {age}인 사람의 정보를 JSON으로 알려주세요. 사는 도시는 서울입니다.\") # 질문 수정: JSON 출력 명시\n",
    "])\n",
    "\n",
    "# 체인 정의: prompt -> model -> parser\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# 체인 실행\n",
    "# LLM이 예상하는 JSON 스키마와 일치하도록 입력을 제공합니다.\n",
    "input_data = {\"name\": \"김철수\", \"age\": 30, \"format_instructions\": parser.get_format_instructions()}\n",
    "parsed_output = chain.invoke(input_data)\n",
    "\n",
    "print(f\"입력 데이터: {input_data}\")\n",
    "print(f\"파싱된 출력 (Type: {type(parsed_output)}): {parsed_output}\")\n",
    "# 파싱된 출력에서 개별 필드에 접근 가능\n",
    "# 딕셔너리 접근 방식으로 수정\n",
    "print(f\"이름: {parsed_output['name']}, 나이: {parsed_output['age']}, 도시: {parsed_output['city']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03bcea",
   "metadata": {},
   "source": [
    "### 2-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05faecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Groq 모델 설정 (이전 노트북 파일에서 사용된 설정 재활용)\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# 첫 번째 체인: 날씨 정보 요청\n",
    "weather_prompt = PromptTemplate.from_template(\n",
    "    \"오늘 {city}의 날씨는 어떤가요? 날씨 정보를 요약해서 알려주세요.\"\n",
    ")\n",
    "weather_chain = {\"city\": RunnablePassthrough()} | weather_prompt | model | StrOutputParser()\n",
    "\n",
    "# 두 번째 체인: 뉴스 요약 요청\n",
    "news_prompt = PromptTemplate.from_template(\n",
    "    \"오늘의 주요 {category} 뉴스에 대해 요약해서 알려주세요. 3문장 이내로.\"\n",
    ")\n",
    "news_chain = {\"category\": RunnablePassthrough()} | news_prompt | model | StrOutputParser()\n",
    "\n",
    "# RunnableParallel을 사용하여 두 체인을 병렬로 실행\n",
    "# 각 체인에 필요한 인풋을 정의합니다.\n",
    "full_chain = RunnableParallel(\n",
    "    current_weather=weather_chain,\n",
    "    today_news=news_chain\n",
    ")\n",
    "\n",
    "# 체인 실행 (두 체인 모두에 필요한 인풋 제공)\n",
    "# 'city'는 weather_chain의 input으로, 'category'는 news_chain의 input으로 들어갑니다.\n",
    "result = full_chain.invoke({\n",
    "    \"city\": \"서울\",\n",
    "    \"category\": \"경제\"\n",
    "})\n",
    "\n",
    "print(\"==== 병렬 처리 결과 ====\")\n",
    "print(f\"서울 날씨: {result['current_weather']}\")\n",
    "print(f\"오늘의 경제 뉴스: {result['today_news']}\")\n",
    "\n",
    "# 다른 예시: 병렬 처리되는 인풋의 키가 다를 경우\n",
    "# RunnableParallel은 키를 매핑해주므로 아래와 같이 정의할 수 있습니다.\n",
    "another_full_chain = RunnableParallel(\n",
    "    seoul_weather=weather_chain.bind(city=\"서울\"), # city 인풋을 고정\n",
    "    tech_news=news_chain.bind(category=\"기술\")    # category 인풋을 고정\n",
    ")\n",
    "\n",
    "# 이 경우 invoke()에 인풋을 전달할 필요가 없습니다.\n",
    "result_fixed_inputs = another_full_chain.invoke({})\n",
    "\n",
    "print(\"\\n==== 고정된 인풋 병렬 처리 결과 ====\")\n",
    "print(f\"서울 날씨: {result_fixed_inputs['seoul_weather']}\")\n",
    "print(f\"오늘의 기술 뉴스: {result_fixed_inputs['tech_news']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
