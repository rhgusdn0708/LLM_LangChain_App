{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a6b78d4",
   "metadata": {},
   "source": [
    "### 3-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90db1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- 중요 수정 사항 1: OpenAI 임베딩 및 LLM 임포트 경로 변경 ---\n",
    "# 기존: from langchain.embeddings import OpenAIEmbeddings\n",
    "# 기존: from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0. 환경 설정 (Environment Setup)\n",
    "#    - .env 파일에서 API 키 로드\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"0. 환경 설정 시작...\")\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY 환경 변수가 설정되지 않았습니다. .env 파일을 확인하거나, API 키를 직접 코드에 설정하세요.\")\n",
    "\n",
    "print(\"환경 설정 완료.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. 문서 로딩 (Document Loading)\n",
    "#    - '콘텐츠분쟁해결_사례.pdf' 파일 로드\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n1. 문서 로딩 시작...\")\n",
    "\n",
    "# --- 중요 수정 사항 2: PDF 파일 경로 수정 ---\n",
    "# PDF 파일이 'C:\\mylangchain\\langchain_basic\\data' 폴더에 있다고 가정\n",
    "pdf_filepath = r'C:\\mylangchain\\langchain_basic\\data\\콘텐츠분쟁해결_사례.pdf'\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "if not os.path.exists(pdf_filepath):\n",
    "    print(f\"오류: '{pdf_filepath}' 파일을 찾을 수 없습니다. 파일 경로를 다시 확인해주세요.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    # --- 중요 수정 사항 3: PyPDFLoader에 file_path 인자 전달 ---\n",
    "    loader = PyPDFLoader(pdf_filepath)\n",
    "    # -------------------------------------------------------------\n",
    "    documents = loader.load()\n",
    "    print(f\"'{pdf_filepath}'에서 {len(documents)} 페이지 문서를 성공적으로 로드했습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"PDF 로딩 중 오류 발생: {e}\")\n",
    "    exit()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. 텍스트 분할 (Text Splitting)\n",
    "#    - 로드된 문서를 작은 청크로 분할\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n2. 텍스트 분할 시작...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"문서를 {len(texts)}개의 청크로 분할했습니다.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. 임베딩 생성 (Generating Embeddings)\n",
    "#    - 분할된 텍스트 청크를 벡터로 변환\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n3. 임베딩 생성 시작...\")\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "print(\"OpenAI 임베딩 모델 로드 완료.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. 벡터 스토어 구축 (Building Vector Store - FAISS)\n",
    "#    - 임베딩된 청크를 FAISS에 저장하여 검색 가능하게 함\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n4. 벡터 스토어 구축 시작 (FAISS)...\")\n",
    "vector_store = FAISS.from_documents(texts, embeddings_model)\n",
    "print(\"FAISS 벡터 스토어 구축 완료.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. 검색기 설정 (Setting up Retriever)\n",
    "#    - 벡터 스토어를 기반으로 한 검색기 생성\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n5. 검색기 설정 시작...\")\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"검색기 설정 완료 (상위 3개 문서 검색).\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. 언어 모델 (LLM) 설정 (Setting up LLM)\n",
    "#    - 질문 답변에 사용할 LLM 모델 설정\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n6. 언어 모델 (LLM) 설정 시작...\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0.1)\n",
    "print(\"LLM (gpt-3.5-turbo-0125) 설정 완료.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7. RAG 체인 구축 및 실행 (Building and Running RAG Chain)\n",
    "#    - 검색된 문서와 질문을 LLM에 전달하여 답변 생성\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n7. RAG 체인 구축 및 실행 시작...\")\n",
    "\n",
    "prompt_template = \"\"\"다음 맥락을 사용하여 질문에 답변하세요.\n",
    "만약 맥락에서 답변을 찾을 수 없다면, 모른다고 말하고 질문을 다시 작성하도록 요청하세요.\n",
    "불필요한 정보를 추가하지 마세요. 질문과 맥락을 기반으로 답변을 생성하세요.\n",
    "\n",
    "맥락:\n",
    "{context}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "답변:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "print(\"RAG 체인 구축 완료.\")\n",
    "\n",
    "print(\"\\n-------------------------------------------------------------\")\n",
    "print(\"RAG 시스템이 준비되었습니다. 질문을 입력해주세요. (종료: 'exit')\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"\\n질문: \")\n",
    "    if query.lower() == 'exit':\n",
    "        print(\"RAG 시스템을 종료합니다.\")\n",
    "        break\n",
    "\n",
    "    print(\"답변 생성 중...\")\n",
    "    try:\n",
    "        result = qa_chain.invoke({\"query\": query})\n",
    "        answer = result[\"result\"]\n",
    "        source_docs = result[\"source_documents\"]\n",
    "\n",
    "        print(f\"\\n답변:\\n{'-'*50}\\n{answer}\\n\")\n",
    "\n",
    "        print(\"참조 문서:\")\n",
    "        for i, doc in enumerate(source_docs[:3]):\n",
    "            page_num = doc.metadata.get('page', 'N/A')\n",
    "            source_file = doc.metadata.get('source', 'N/A')\n",
    "            # 페이지 내용의 일부만 보여줌\n",
    "            preview = doc.page_content.replace('\\n', ' ')[:100] + \"...\"\n",
    "            print(f\"  {i+1}. 페이지 {page_num} (파일: {os.path.basename(source_file)}): {preview}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"RAG 체인 실행 중 오류 발생: {e}\")\n",
    "        print(\"API 키 또는 네트워크 연결을 확인해주세요.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
